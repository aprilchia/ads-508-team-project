{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Job Postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADS 508 Impacting the Business with a Distributed Data Science Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "from pyathena import connect\n",
    "import awswrangler as wr\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup boto3 session parameters\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "# Establish connection\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set S3 Source Location (Public bucket)\n",
    "s3_public_path = \"s3://linkedin-postings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_public_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set S3 Destination Location (Private bucket)\n",
    "s3_private_path = \"s3://{}/linkedin_data\".format(bucket)\n",
    "print(s3_private_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_private_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copy data from Public S3 bucket to Private S3 bucket\n",
    "!aws s3 cp --recursive $s3_public_path/ $s3_private_path/ --exclude \"*\" --include \"postings/postings.csv\"\n",
    "!aws s3 cp --recursive $s3_public_path/ $s3_private_path/ --exclude \"*\" --include \"salaries/salaries.csv\"\n",
    "!aws s3 cp --recursive $s3_public_path/ $s3_private_path/ --exclude \"*\" --include \"job_skills/job_skills.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check files are copied successfully to private bucket\n",
    "!aws s3 ls $s3_private_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and paths \n",
    "bucket_name = bucket\n",
    "file_key = 'linkedin_data/postings/postings.csv'\n",
    "cleaned_file_key = 'linkedin_data/postings/cleaned/cleaned_postings.csv'\n",
    "\n",
    "# Read postings.csv directly from private bucket\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "# Remove embedded newlines\n",
    "df['description'].replace({r'[\\n\\r]+': ' '}, regex=True, inplace=True)\n",
    "df['description'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "df['skills_desc'].replace({r'[\\n\\r]+': ' '}, regex=True, inplace=True)\n",
    "df['skills_desc'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "\n",
    "# Save cleaned CSV back to S3 directly (in-memory)\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=cleaned_file_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Cleaned CSV successfully uploaded to: s3://{bucket_name}/{cleaned_file_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Athena Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_create_athena_db_passed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_create_athena_table_passed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "database_name = \"linkedin_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set S3 staging directory - a temporary directory for Athena queries\n",
    "s3_staging_dir = \"s3://{}/athena/staging\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to staging directory\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Database\n",
    "statement = \"CREATE DATABASE IF NOT EXISTS {}\".format(database_name)\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify database has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statement = \"SHOW DATABASES\"\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if database_name in df_show.values:\n",
    "    ingest_create_athena_db_passed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Athena Tables from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'postings'\n",
    "postings_path = \"s3://{}/linkedin_data/postings/cleaned/\".format(bucket)\n",
    "\n",
    "drop_statement = \"\"\"DROP TABLE IF EXISTS {}.{};\"\"\".format(database_name, table_name)\n",
    "\n",
    "print(drop_statement)\n",
    "pd.read_sql(drop_statement, conn)\n",
    "print(\"Attempted to Drop {} table\".format(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SQL statement to execute the postings table\n",
    "statement = \"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "        job_id string,\n",
    "        company_name string,\n",
    "        title string,\n",
    "        description string,\n",
    "        max_salary float,\n",
    "        pay_period string,\n",
    "        location string,\n",
    "        company_id float,\n",
    "        views float,\n",
    "        med_salary float,\n",
    "        min_salary float,\n",
    "        formatted_work_type string,\n",
    "        applies float,\n",
    "        original_listed_time float,\n",
    "        remote_allowed float,\n",
    "        job_posting_url string,\n",
    "        application_url string,\n",
    "        application_type string,\n",
    "        expiry float,\n",
    "        closed_time float,\n",
    "        formatted_experience_level string,\n",
    "        skills_desc string,\n",
    "        listed_time string,\n",
    "        posting_domain string,\n",
    "        sponsored int,\n",
    "        work_type string,\n",
    "        currency string,\n",
    "        compensation_type string,\n",
    "        normalized_salary float,\n",
    "        zip_code int,\n",
    "        fips int\n",
    "    ) \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    LOCATION '{}' \n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "    \"\"\".format(database_name, table_name, postings_path)\n",
    "\n",
    "# Execute statement\n",
    "pd.read_sql(statement, conn)\n",
    "print(\"Created postings table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_2 = \"salaries\"\n",
    "salaries_path = \"s3://{}/linkedin_data/salaries/\".format(bucket)\n",
    "\n",
    "drop_statement2 = \"\"\"DROP TABLE IF EXISTS {}.{};\"\"\".format(database_name, table_name_2)\n",
    "\n",
    "print(drop_statement2)\n",
    "pd.read_sql(drop_statement2, conn)\n",
    "print(\"Attempted to Drop {} table\".format(table_name_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SQL statement to execute the postings table\n",
    "statement = \"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "        salary_id int,\n",
    "        job_id string,\n",
    "        max_salary float,\n",
    "        med_salary float,\n",
    "        min_salary float,\n",
    "        pay_period string,\n",
    "        currency string,\n",
    "        compensation_type string\n",
    "    ) \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    LOCATION '{}' \n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "    \"\"\".format(database_name, table_name_2, salaries_path)\n",
    "\n",
    "# Execute statement\n",
    "pd.read_sql(statement, conn)\n",
    "print(\"Created salaries table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_3 = \"job_skills\"\n",
    "job_skills_path = \"s3://{}/linkedin_data/job_skills/\".format(bucket)\n",
    "\n",
    "drop_statement3 = \"\"\"DROP TABLE IF EXISTS {}.{};\"\"\".format(database_name, table_name_3)\n",
    "\n",
    "print(drop_statement3)\n",
    "pd.read_sql(drop_statement3, conn)\n",
    "print(\"Attempted to Drop {} table\".format(table_name_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SQL statement to execute the postings table\n",
    "statement = \"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "        job_id string,\n",
    "        skill_abr string\n",
    "    ) \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    LOCATION '{}' \n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "    \"\"\".format(database_name, table_name_3, job_skills_path)\n",
    "\n",
    "# Execute statement\n",
    "pd.read_sql(statement, conn)\n",
    "print(\"Created job_skills table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Athena Parquet Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postings Parquet Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"postings\"\n",
    "table_name_parquet = \"postings_parquet\"\n",
    "postings_parquet_path = \"s3://{}/linkedin_data/parquet/postings/\".format(bucket)\n",
    "\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}') AS\n",
    "SELECT job_id,\n",
    "        company_name,\n",
    "        title,\n",
    "        description,\n",
    "        max_salary,\n",
    "        pay_period,\n",
    "        location,\n",
    "        company_id,\n",
    "        views,\n",
    "        med_salary,\n",
    "        min_salary,\n",
    "        formatted_work_type,\n",
    "        applies,\n",
    "        original_listed_time,\n",
    "        remote_allowed,\n",
    "        job_posting_url,\n",
    "        application_url,\n",
    "        application_type,\n",
    "        expiry,\n",
    "        closed_time,\n",
    "        formatted_experience_level,\n",
    "        skills_desc,\n",
    "        listed_time,\n",
    "        posting_domain,\n",
    "        sponsored,\n",
    "        work_type,\n",
    "        currency,\n",
    "        compensation_type,\n",
    "        normalized_salary,\n",
    "        zip_code,\n",
    "        fips\n",
    "FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name_parquet, postings_parquet_path, database_name, table_name\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salaries Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"salaries\"\n",
    "table_name_parquet = \"salaries_parquet\"\n",
    "salaries_parquet_path = \"s3://{}/linkedin_data/parquet/salaries/\".format(bucket)\n",
    "\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}') AS\n",
    "SELECT salary_id,\n",
    "    job_id,\n",
    "    max_salary,\n",
    "    med_salary,\n",
    "    min_salary,\n",
    "    pay_period,\n",
    "    currency,\n",
    "    compensation_type\n",
    "FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name_parquet, salaries_parquet_path, database_name, table_name\n",
    ")\n",
    "\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Skills Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"job_skills\"\n",
    "table_name_parquet = \"job_skills_parquet\"\n",
    "job_skills_parquet_path = \"s3://{}/linkedin_data/parquet/job_skills/\".format(bucket)\n",
    "\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}') AS\n",
    "SELECT job_id,\n",
    "        skill_abr\n",
    "FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name_parquet, job_skills_parquet_path, database_name, table_name\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify tables have been created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statement = \"SHOW TABLES in {}\".format(database_name)\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if table_name in df_show.values:\n",
    "    ingest_create_athena_table_passed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View postings table to check the data looks correct\n",
    "statement = \"\"\"SELECT * FROM {}.{} LIMIT 5\"\"\".format(database_name, 'postings_parquet')\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# View salaries table to check the data looks correct\n",
    "statement = \"\"\"SELECT * FROM {}.{} LIMIT 5\"\"\".format(database_name, 'salaries_parquet')\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View job_skills table to check the data looks correct\n",
    "statement = \"\"\"SELECT * FROM {}.{} LIMIT 5\"\"\".format(database_name, 'job_skills_parquet')\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_df = wr.athena.read_sql_query(\"\"\"SELECT * FROM postings_parquet\"\"\", database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df = wr.athena.read_sql_query(\"\"\"SELECT * FROM salaries_parquet\"\"\", database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_skills_df = wr.athena.read_sql_query(\"\"\"SELECT * FROM job_skills_parquet\"\"\", database=database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postings EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic info\n",
    "postings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "postings_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "duplicates = postings_df.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(postings_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 unique values in categorical columns\n",
    "categorical = [\"title\", \"zip_code\", \"formatted_work_type\", \"remote_allowed\"]\n",
    "for col in categorical:\n",
    "    print(postings_df[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Outliers using box plots\n",
    "numerical = [\"views\", \"applies\"]\n",
    "for col in numerical:\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.boxplot(x=postings_df[col])\n",
    "    plt.title(f\"Outlier Detection - {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize experience level distribution\n",
    "top_titles = postings_df[\"formatted_experience_level\"].value_counts().iloc[[1,3,7]]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(y=top_titles.index, x=top_titles.values, palette=\"coolwarm\")\n",
    "plt.xlabel(\"Number of Postings\")\n",
    "plt.ylabel(\"Experience Level\")\n",
    "plt.title(\"Job Postings by Experience Level\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### salaries EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic stats\n",
    "print(salaries_df.describe())\n",
    "print(salaries_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "salaries_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "salaries_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Outliers using box plots\n",
    "numerical = [\"max_salary\", \"med_salary\", \"min_salary\"]\n",
    "for col in numerical:\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.boxplot(x=postings_df[col])\n",
    "    plt.title(f\"Outlier Detection - {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Salary distributions\n",
    "salary_columns = [\"min_salary\", \"med_salary\", \"max_salary\"]\n",
    "print(salaries_df[salary_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df.hist(figsize=(18,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(salaries_df[\"min_salary\"], color=\"blue\", kde=True, label=\"Min Salary\", bins=30)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Salary Distribution (Histogram)\")\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(salaries_df[\"med_salary\"], color=\"green\", kde=True, label=\"Median Salary\", bins=30)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Salary Distribution (Histogram)\")\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(salaries_df[\"max_salary\"], color=\"red\", kde=True, label=\"Max Salary\", bins=30)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Salary Distribution (Histogram)\")\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### job_skills EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(job_skills_df.describe())\n",
    "print(job_skills_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "job_skills_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "job_skills_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unique skills\n",
    "print(f\"Unique Job Skills: {job_skills_df['skill_abr'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Skill frequencies\n",
    "job_skills_df[\"skill_abr\"] = job_skills_df[\"skill_abr\"].astype(str)  # Ensure it's a string\n",
    "skills = job_skills_df[\"skill_abr\"].str.split(\",\").explode().str.strip().value_counts()\n",
    "print(skills.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all skills descriptions\n",
    "all_skills = \" \".join(job_skills_df[\"skill_abr\"].dropna())\n",
    "\n",
    "# Generate WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(all_skills)\n",
    "\n",
    "# Bar Chart of Top 20 Most Common Skills\n",
    "top_skills = skills.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(y=top_skills.index, x=top_skills.values, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Most Common Skills\")\n",
    "plt.xlabel(\"Number of Occurrences\")\n",
    "plt.ylabel(\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values - postings_df\n",
    "postings_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values - salaries_df\n",
    "salaries_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values - job_skills_df\n",
    "job_skills_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Romove unnecessary columns\n",
    "columns_to_keep = ['job_id', 'title', 'pay_period', 'remote_allowed', 'formatted_work_type', 'zip_code']\n",
    "postings_df = postings_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Filter for U.S. postings only\n",
    "us_states = [\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', \n",
    "    'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "    'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "    'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "    ]\n",
    "\n",
    "def us_location(location):\n",
    "    if pd.isna(location):  # Handle NaN values\n",
    "        return False\n",
    "    \n",
    "    parts = [part.strip() for part in location.split(',')]  # Split by comma and clean spaces\n",
    "    \n",
    "    if any(part.lower() in ['usa', 'united states'] for part in parts):\n",
    "        return True\n",
    "    \n",
    "    if any(part in us_states for part in parts):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "postings_df = postings_df[postings_df['location'].apply(us_location)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with median of respective column\n",
    "postings_df['max_salary'].fillna(postings_df['max_salary'].median(), inplace=True)\n",
    "postings_df['min_salary'].fillna(postings_df['min_salary'].median(), inplace=True)\n",
    "postings_df['med_salary'].fillna(postings_df['med_salary'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no incorrect data types for salaries\n",
    "postings_df['max_salary'] = pd.to_numeric(postings_df['max_salary'], errors='coerce')\n",
    "postings_df['min_salary'] = pd.to_numeric(postings_df['min_salary'], errors='coerce')\n",
    "postings_df['med_salary'] = pd.to_numeric(postings_df['med_salary'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values in pay_period column\n",
    "postings_df = postings_df.dropna(subset=['pay_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in empty cells with 0 for remote NOT allowed\n",
    "postings_df['remote_allowed'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with the most common experience level\n",
    "postings_df['formatted_experience_level'].fillna(postings_df['formatted_experience_level'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed .0 at the end of all zipcodes and filled NaN with 0\n",
    "postings_df['zip_code'] = postings_df['zip_code'].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check df looks right\n",
    "postings_df.info()\n",
    "postings_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with median of respective column\n",
    "salaries_df['max_salary'].fillna(salaries_df['max_salary'].median(), inplace=True)\n",
    "salaries_df['min_salary'].fillna(salaries_df['min_salary'].median(), inplace=True)\n",
    "salaries_df['med_salary'].fillna(salaries_df['med_salary'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no incorrect data types for salaries\n",
    "salaries_df['max_salary'] = pd.to_numeric(salaries_df['max_salary'], errors='coerce')\n",
    "salaries_df['min_salary'] = pd.to_numeric(salaries_df['min_salary'], errors='coerce')\n",
    "salaries_df['med_salary'] = pd.to_numeric(salaries_df['med_salary'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inner join all dfs for modeling?\n",
    "# df = postings_df.merge(job_skills_df, on='job_id', how='inner')\n",
    "# df = df.merge(salaries_df, on='job_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
