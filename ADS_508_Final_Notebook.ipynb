{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Job Postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADS 508 Impacting the Business with a Distributed Data Science Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "from pyathena import connect\n",
    "import awswrangler as wr\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup boto3 session parameters\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "# Establish connection\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set S3 Source Location (Public bucket)\n",
    "s3_public_path = \"s3://linkedin-postings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_public_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set S3 Destination Location (Private bucket)\n",
    "s3_private_path = \"s3://{}/linkedin_data\".format(bucket)\n",
    "print(s3_private_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_private_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copy data from Public S3 bucket to Private S3 bucket\n",
    "!aws s3 cp --recursive $s3_public_path/ $s3_private_path/ --exclude \"*\" --include \"postings/postings.csv\"\n",
    "!aws s3 cp --recursive $s3_public_path/ $s3_private_path/ --exclude \"*\" --include \"salaries/salaries.csv\"\n",
    "!aws s3 cp --recursive $s3_public_path/ $s3_private_path/ --exclude \"*\" --include \"job_skills/job_skills.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check files are copied successfully to private bucket\n",
    "!aws s3 ls $s3_private_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and paths \n",
    "bucket_name = bucket\n",
    "file_key = 'linkedin_data/postings/postings.csv'\n",
    "cleaned_file_key = 'linkedin_data/postings/cleaned/cleaned_postings.csv'\n",
    "\n",
    "# Read postings.csv directly from private bucket\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "# Remove embedded newlines\n",
    "df['description'].replace({r'[\\n\\r]+': ' '}, regex=True, inplace=True)\n",
    "df['skills_desc'].replace({r'[\\n\\r]+': ' '}, regex=True, inplace=True)\n",
    "\n",
    "# Remove embedded commas\n",
    "df['company_name'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "df['title'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "df['description'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "df['location'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "df['skills_desc'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "\n",
    "\n",
    "# Save cleaned CSV back to S3 directly (in-memory)\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=cleaned_file_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Cleaned CSV successfully uploaded to: s3://{bucket_name}/{cleaned_file_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Athena Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_create_athena_db_passed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_create_athena_table_passed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "database_name = \"linkedin_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set S3 staging directory - a temporary directory for Athena queries\n",
    "s3_staging_dir = \"s3://{}/athena/staging\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to staging directory\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Database\n",
    "statement = \"CREATE DATABASE IF NOT EXISTS {}\".format(database_name)\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify database has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statement = \"SHOW DATABASES\"\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if database_name in df_show.values:\n",
    "    ingest_create_athena_db_passed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Athena Tables from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'postings'\n",
    "postings_path = \"s3://{}/linkedin_data/postings/cleaned/\".format(bucket)\n",
    "\n",
    "drop_statement = \"\"\"DROP TABLE IF EXISTS {}.{};\"\"\".format(database_name, table_name)\n",
    "\n",
    "print(drop_statement)\n",
    "pd.read_sql(drop_statement, conn)\n",
    "print(\"Attempted to Drop {} table\".format(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SQL statement to execute the postings table\n",
    "statement = \"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "        job_id string,\n",
    "        company_name string,\n",
    "        title string,\n",
    "        description string,\n",
    "        max_salary float,\n",
    "        pay_period string,\n",
    "        location string,\n",
    "        company_id float,\n",
    "        views float,\n",
    "        med_salary float,\n",
    "        min_salary float,\n",
    "        formatted_work_type string,\n",
    "        applies float,\n",
    "        original_listed_time float,\n",
    "        remote_allowed float,\n",
    "        job_posting_url string,\n",
    "        application_url string,\n",
    "        application_type string,\n",
    "        expiry float,\n",
    "        closed_time float,\n",
    "        formatted_experience_level string,\n",
    "        skills_desc string,\n",
    "        listed_time string,\n",
    "        posting_domain string,\n",
    "        sponsored int,\n",
    "        work_type string,\n",
    "        currency string,\n",
    "        compensation_type string,\n",
    "        normalized_salary float,\n",
    "        zip_code int,\n",
    "        fips int\n",
    "    ) \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    LOCATION '{}' \n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "    \"\"\".format(database_name, table_name, postings_path)\n",
    "\n",
    "# Execute statement\n",
    "pd.read_sql(statement, conn)\n",
    "print(\"Created postings table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_2 = \"salaries\"\n",
    "salaries_path = \"s3://{}/linkedin_data/salaries/\".format(bucket)\n",
    "\n",
    "drop_statement2 = \"\"\"DROP TABLE IF EXISTS {}.{};\"\"\".format(database_name, table_name_2)\n",
    "\n",
    "print(drop_statement2)\n",
    "pd.read_sql(drop_statement2, conn)\n",
    "print(\"Attempted to Drop {} table\".format(table_name_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SQL statement to execute the postings table\n",
    "statement = \"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "        salary_id int,\n",
    "        job_id string,\n",
    "        max_salary float,\n",
    "        med_salary float,\n",
    "        min_salary float,\n",
    "        pay_period string,\n",
    "        currency string,\n",
    "        compensation_type string\n",
    "    ) \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    LOCATION '{}' \n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "    \"\"\".format(database_name, table_name_2, salaries_path)\n",
    "\n",
    "# Execute statement\n",
    "pd.read_sql(statement, conn)\n",
    "print(\"Created salaries table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_3 = \"job_skills\"\n",
    "job_skills_path = \"s3://{}/linkedin_data/job_skills/\".format(bucket)\n",
    "\n",
    "drop_statement3 = \"\"\"DROP TABLE IF EXISTS {}.{};\"\"\".format(database_name, table_name_3)\n",
    "\n",
    "print(drop_statement3)\n",
    "pd.read_sql(drop_statement3, conn)\n",
    "print(\"Attempted to Drop {} table\".format(table_name_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SQL statement to execute the postings table\n",
    "statement = \"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "        job_id string,\n",
    "        skill_abr string\n",
    "    ) \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    LOCATION '{}' \n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "    \"\"\".format(database_name, table_name_3, job_skills_path)\n",
    "\n",
    "# Execute statement\n",
    "pd.read_sql(statement, conn)\n",
    "print(\"Created job_skills table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Athena Parquet Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postings Parquet Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'postings_parquet'\n",
    "drop_statement = \"\"\"DROP TABLE IF EXISTS {}.{};\"\"\".format(database_name, table_name)\n",
    "\n",
    "print(drop_statement)\n",
    "pd.read_sql(drop_statement, conn)\n",
    "print(\"Attempted to Drop {} table\".format(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"postings\"\n",
    "table_name_parquet = \"postings_parquet\"\n",
    "postings_parquet_path = \"s3://{}/linkedin_data/parquet/postings/\".format(bucket)\n",
    "\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}') AS\n",
    "SELECT job_id,\n",
    "        company_name,\n",
    "        title,\n",
    "        description,\n",
    "        max_salary,\n",
    "        pay_period,\n",
    "        location,\n",
    "        company_id,\n",
    "        views,\n",
    "        med_salary,\n",
    "        min_salary,\n",
    "        formatted_work_type,\n",
    "        applies,\n",
    "        original_listed_time,\n",
    "        remote_allowed,\n",
    "        job_posting_url,\n",
    "        application_url,\n",
    "        application_type,\n",
    "        expiry,\n",
    "        closed_time,\n",
    "        formatted_experience_level,\n",
    "        skills_desc,\n",
    "        listed_time,\n",
    "        posting_domain,\n",
    "        sponsored,\n",
    "        work_type,\n",
    "        currency,\n",
    "        compensation_type,\n",
    "        normalized_salary,\n",
    "        zip_code,\n",
    "        fips\n",
    "FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name_parquet, postings_parquet_path, database_name, table_name\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salaries Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"salaries\"\n",
    "table_name_parquet = \"salaries_parquet\"\n",
    "salaries_parquet_path = \"s3://{}/linkedin_data/parquet/salaries/\".format(bucket)\n",
    "\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}') AS\n",
    "SELECT salary_id,\n",
    "    job_id,\n",
    "    max_salary,\n",
    "    med_salary,\n",
    "    min_salary,\n",
    "    pay_period,\n",
    "    currency,\n",
    "    compensation_type\n",
    "FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name_parquet, salaries_parquet_path, database_name, table_name\n",
    ")\n",
    "\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Skills Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"job_skills\"\n",
    "table_name_parquet = \"job_skills_parquet\"\n",
    "job_skills_parquet_path = \"s3://{}/linkedin_data/parquet/job_skills/\".format(bucket)\n",
    "\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}') AS\n",
    "SELECT job_id,\n",
    "        skill_abr\n",
    "FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name_parquet, job_skills_parquet_path, database_name, table_name\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify tables have been created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statement = \"SHOW TABLES in {}\".format(database_name)\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if table_name in df_show.values:\n",
    "    ingest_create_athena_table_passed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View postings table to check the data looks correct\n",
    "statement = \"\"\"SELECT * FROM {}.{} LIMIT 5\"\"\".format(database_name, 'postings_parquet')\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# View salaries table to check the data looks correct\n",
    "statement = \"\"\"SELECT * FROM {}.{} LIMIT 5\"\"\".format(database_name, 'salaries_parquet')\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View job_skills table to check the data looks correct\n",
    "statement = \"\"\"SELECT * FROM {}.{} LIMIT 5\"\"\".format(database_name, 'job_skills_parquet')\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_df = wr.athena.read_sql_query(\"\"\"SELECT * FROM postings_parquet\"\"\", database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df = wr.athena.read_sql_query(\"\"\"SELECT * FROM salaries_parquet\"\"\", database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_skills_df = wr.athena.read_sql_query(\"\"\"SELECT * FROM job_skills_parquet\"\"\", database=database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postings EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic info\n",
    "postings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "postings_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "duplicates = postings_df.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(postings_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 unique values in categorical columns\n",
    "categorical = [\"title\", \"zip_code\", \"formatted_work_type\", \"remote_allowed\"]\n",
    "for col in categorical:\n",
    "    print(postings_df[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Outliers using box plots\n",
    "numerical = [\"views\", \"applies\"]\n",
    "for col in numerical:\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.boxplot(x=postings_df[col])\n",
    "    plt.title(f\"Outlier Detection - {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize experience level distribution\n",
    "top_titles = postings_df[\"formatted_experience_level\"].value_counts().iloc[[1,3,7]]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(y=top_titles.index, x=top_titles.values, palette=\"coolwarm\")\n",
    "plt.xlabel(\"Number of Postings\")\n",
    "plt.ylabel(\"Experience Level\")\n",
    "plt.title(\"Job Postings by Experience Level\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### salaries EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic stats\n",
    "print(salaries_df.describe())\n",
    "print(salaries_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "salaries_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "salaries_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Outliers using box plots\n",
    "numerical = [\"max_salary\", \"med_salary\", \"min_salary\"]\n",
    "for col in numerical:\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.boxplot(x=postings_df[col])\n",
    "    plt.title(f\"Outlier Detection - {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Salary distributions\n",
    "salary_columns = [\"min_salary\", \"med_salary\", \"max_salary\"]\n",
    "print(salaries_df[salary_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df.hist(figsize=(18,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(salaries_df[\"min_salary\"], color=\"blue\", kde=True, label=\"Min Salary\", bins=30)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Salary Distribution (Histogram)\")\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(salaries_df[\"med_salary\"], color=\"green\", kde=True, label=\"Median Salary\", bins=30)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Salary Distribution (Histogram)\")\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(salaries_df[\"max_salary\"], color=\"red\", kde=True, label=\"Max Salary\", bins=30)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Salary Distribution (Histogram)\")\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### job_skills EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(job_skills_df.describe())\n",
    "print(job_skills_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values\n",
    "job_skills_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "job_skills_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unique skills\n",
    "print(f\"Unique Job Skills: {job_skills_df['skill_abr'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Skill frequencies\n",
    "job_skills_df[\"skill_abr\"] = job_skills_df[\"skill_abr\"].astype(str)  # Ensure it's a string\n",
    "skills = job_skills_df[\"skill_abr\"].str.split(\",\").explode().str.strip().value_counts()\n",
    "print(skills.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all skills descriptions\n",
    "all_skills = \" \".join(job_skills_df[\"skill_abr\"].dropna())\n",
    "\n",
    "# Generate WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(all_skills)\n",
    "\n",
    "# Bar Chart of Top 20 Most Common Skills\n",
    "top_skills = skills.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(y=top_skills.index, x=top_skills.values, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Most Common Skills\")\n",
    "plt.xlabel(\"Number of Occurrences\")\n",
    "plt.ylabel(\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values - postings_df\n",
    "postings_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values - salaries_df\n",
    "salaries_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Missing values - job_skills_df\n",
    "job_skills_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "columns_to_keep = ['job_id', 'title', 'pay_period', 'remote_allowed', 'formatted_work_type', 'zip_code']\n",
    "postings_df = postings_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values in pay_period column\n",
    "postings_df = postings_df.dropna(subset=['pay_period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in empty cells with 0 for remote NOT allowed\n",
    "postings_df['remote_allowed'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed .0 at the end of all zipcodes and filled NaN with 0\n",
    "postings_df['zip_code'] = postings_df['zip_code'].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check df looks right\n",
    "postings_df.info()\n",
    "postings_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with median of respective column\n",
    "salaries_df['max_salary'].fillna(salaries_df['max_salary'].median(), inplace=True)\n",
    "salaries_df['min_salary'].fillna(salaries_df['min_salary'].median(), inplace=True)\n",
    "salaries_df['med_salary'].fillna(salaries_df['med_salary'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no incorrect data types for salaries\n",
    "salaries_df['max_salary'] = pd.to_numeric(salaries_df['max_salary'], errors='coerce')\n",
    "salaries_df['min_salary'] = pd.to_numeric(salaries_df['min_salary'], errors='coerce')\n",
    "salaries_df['med_salary'] = pd.to_numeric(salaries_df['med_salary'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write pre-processed df to S3\n",
    "csv_buffer = StringIO()\n",
    "postings_df.to_csv(csv_buffer, index=False)\n",
    "file_key = 'linkedin_data/postings/preprocessing/postings.csv'\n",
    "\n",
    "s3.put_object(Bucket=bucket, Key=file_key, Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the processor instance\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the input data\n",
    "key_file = 'linkedin_data/postings/preprocessing/postings.csv'\n",
    "input_data = 's3://{}/{}'.format(bucket, key_file)\n",
    "obj = s3.get_object(Bucket=bucket, Key=key_file)\n",
    "df = pd.read_csv(obj['Body'], nrows=10)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py \n",
    "import os\n",
    "\n",
    "os.system('pip install sentence-transformers')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    categorical_columns = ['zip_code', 'formatted_work_type', 'remote_allowed', 'pay_period']\n",
    "    \n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\") # SBERT, not case-sensitive\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'postings.csv')\n",
    "\n",
    "    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    \n",
    "    # create embeddings for 'title'\n",
    "    print('Creating embeddings for job title')\n",
    "    df[\"title_embeddings\"] = df[\"title\"].apply(lambda x: model.encode(x))\n",
    "\n",
    "    # Use PCA to reduce the embeddings vector down to 2 dimensions \n",
    "    print('Reducing embeddings to 2 dimensions')\n",
    "    pca = PCA(n_components=2, svd_solver='full')\n",
    "    embeddings_matrix = np.vstack(df['title_embeddings'].values)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings_matrix)\n",
    "    \n",
    "    df['pca_1'] = reduced_embeddings[:, 0]\n",
    "    df['pca_2'] = reduced_embeddings[:, 1]\n",
    "\n",
    "    # One hot encode categorical fields\n",
    "    df = pd.get_dummies(df, columns=categorical_columns)\n",
    "    \n",
    "    # save csv to output_path\n",
    "    output_path = os.path.join('/opt/ml/processing/output', 'postings.csv')\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the processing job\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    inputs=[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"processed_postings\", source=\"/opt/ml/processing/output\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"processed_postings\":\n",
    "        preprocessed_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "        \n",
    "        # check output of processing job\n",
    "        df = pd.read_csv(preprocessed_data + \"/postings.csv\", nrows=10)\n",
    "        df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: 80% train, 20% temp\n",
    "df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Split the remaining 20% evenly\n",
    "df_validate, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Confirm the splits\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Validate shape:\", df_validate.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Use your private bucket from earlier\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = bucket  # this should already be set\n",
    "base_path = 'linkedin_data/partition'\n",
    "\n",
    "# Upload helper function\n",
    "def upload_df_to_s3(df, key):\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, index=False)\n",
    "    s3.put_object(Bucket=bucket_name, Key=key, Body=buffer.getvalue())\n",
    "    print(f\"✅ Uploaded to s3://{bucket_name}/{key}\")\n",
    "\n",
    "# Upload each partition\n",
    "upload_df_to_s3(df_train, f'{base_path}/train/train.csv')\n",
    "upload_df_to_s3(df_validate, f'{base_path}/validate/validate.csv')\n",
    "upload_df_to_s3(df_test, f'{base_path}/test/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from S3 into pandas\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(\n",
    "    Bucket= session.default_bucket(),\n",
    "    Key='linkedin_data/partition/train/train.csv'\n",
    ")\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "# Convert to NumPy\n",
    "train_np = df.to_numpy()\n",
    "\n",
    "# Record shape\n",
    "num_records = train_np.shape[0]\n",
    "feature_dim = train_np.shape[1]\n",
    "\n",
    "print(\"Training shape:\", train_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your preprocessing\n",
    "df_numeric = df.select_dtypes(include=['int64', 'float64', 'bool'])\n",
    "train_np = df_numeric.astype('float32').to_numpy()\n",
    "\n",
    "# Insert this\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker import KMeans\n",
    "\n",
    "container = retrieve('kmeans', region=region)\n",
    "\n",
    "kmeans = KMeans(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    k=5,\n",
    "    output_path=f's3://{bucket}/output/kmeans/',\n",
    "    sagemaker_session=session,\n",
    "    image_uri=container,\n",
    "    mini_batch_size=max(500, int(num_records * 0.05))\n",
    ")\n",
    "\n",
    "# Now this will work\n",
    "train_data = kmeans.record_set(train_np)\n",
    "kmeans.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the trained KMeans model\n",
    "kmeans_predictor = kmeans.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Set serialization for inference\n",
    "kmeans_predictor.serializer = CSVSerializer()\n",
    "kmeans_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "def predict_in_batches(predictor, data, batch_size=1000):\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        result = predictor.predict(batch)\n",
    "        batch_preds = [int(pred['closest_cluster']) for pred in result['predictions']]\n",
    "        predictions.extend(batch_preds)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict cluster assignments in batches\n",
    "assignments = predict_in_batches(kmeans_predictor, input_data)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "df_with_clusters = df.copy()\n",
    "df_with_clusters['cluster'] = assignments\n",
    "\n",
    "df_with_clusters.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    df_with_clusters['pca_1'], \n",
    "    df_with_clusters['pca_2'], \n",
    "    c=df_with_clusters['cluster'], \n",
    "    cmap='tab10', \n",
    "    alpha=0.6\n",
    ")\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('KMeans Clusters on Job Postings')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "obj_test = s3.get_object(\n",
    "    Bucket= session.default_bucket(),\n",
    "    Key='linkedin_data/partition/test/test.csv'\n",
    ")\n",
    "df_test = pd.read_csv(obj_test['Body'])\n",
    "\n",
    "# Load validation set\n",
    "obj_val = s3.get_object(\n",
    "    Bucket= session.default_bucket(),\n",
    "    Key='linkedin_data/partition/validate/validate.csv'\n",
    ")\n",
    "df_val = pd.read_csv(obj_val['Body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure df_numeric is already defined from training\n",
    "# df_numeric = df.select_dtypes(include=['int64', 'float64', 'bool'])\n",
    "\n",
    "# Keep only numeric features and convert to float32\n",
    "df_test_numeric = df_test[df_numeric.columns].astype('float32')\n",
    "df_val_numeric = df_val[df_numeric.columns].astype('float32')\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "test_input = df_test_numeric.to_numpy()\n",
    "val_input = df_val_numeric.to_numpy()\n",
    "\n",
    "# Sanity check\n",
    "print(\"Test shape:\", test_input.shape, \"| dtype:\", test_input.dtype)\n",
    "print(\"Val shape:\", val_input.shape, \"| dtype:\", val_input.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "kmeans_predictor.serializer = CSVSerializer()\n",
    "kmeans_predictor.deserializer = JSONDeserializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(predictor, data, batch_size=1000):\n",
    "    predictions = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        result = predictor.predict(batch)\n",
    "        batch_preds = [int(pred['closest_cluster']) for pred in result['predictions']]\n",
    "        predictions.extend(batch_preds)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assignments = predict_in_batches(kmeans_predictor, test_input, batch_size=250)\n",
    "val_assignments = predict_in_batches(kmeans_predictor, val_input, batch_size=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['cluster'] = test_assignments\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['cluster'] = val_assignments\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(df, key):\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, index=False)\n",
    "    s3.put_object(\n",
    "        Bucket= session.default_bucket(),\n",
    "        Key=key,\n",
    "        Body=buffer.getvalue()\n",
    "    )\n",
    "    print(f\"✅ Uploaded to s3://private-bucket/{key}\")\n",
    "\n",
    "upload_to_s3(df_test, 'linkedin_data/partitions/predictions/test_with_clusters.csv')\n",
    "upload_to_s3(df_val, 'linkedin_data/partitions/predictions/val_with_clusters.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    df_test['pca_1'],\n",
    "    df_test['pca_2'],\n",
    "    c=df_test['cluster'],\n",
    "    cmap='tab10',\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('KMeans Clusters - Test Set')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    df_val['pca_1'],\n",
    "    df_val['pca_2'],\n",
    "    c=df_val['cluster'],\n",
    "    cmap='tab10',\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('KMeans Clusters - Validation Set')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating With Silhouette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Make sure only using numeric columns used in clustering\n",
    "X_train = df_with_clusters[df_numeric.columns]\n",
    "X_test = df_test[df_numeric.columns]\n",
    "X_val = df_val[df_numeric.columns]\n",
    "\n",
    "# Get the cluster assignments\n",
    "labels_train = df_with_clusters['cluster']\n",
    "labels_test = df_test['cluster']\n",
    "labels_val = df_val['cluster']\n",
    "\n",
    "# Calculate silhouette scores\n",
    "score_train = silhouette_score(X_train, labels_train)\n",
    "score_test = silhouette_score(X_test, labels_test)\n",
    "score_val = silhouette_score(X_val, labels_val)\n",
    "\n",
    "print(\"Silhouette Score (Train):\", round(score_train, 4))\n",
    "print(\"Silhouette Score (Test):\", round(score_test, 4))\n",
    "print(\"Silhouette Score (Validation):\", round(score_val, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
