{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de10b69-28be-4e55-82e4-ba63200689a6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84235e1a-a58e-4b0e-848a-e60ed5a897cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import json\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "\n",
    "# load environment variables\n",
    "env_file = '.env'\n",
    "dotenv.load_dotenv(env_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c15a0-3ccc-4ac1-be1d-69bc732d67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('~/Downloads/archive (8)/postings.csv')\n",
    "df.columns\n",
    "\n",
    "# Remove embedded newlines\n",
    "df['description'].replace({r'[\\n\\r]+': ' '}, regex=True, inplace=True)\n",
    "df['description'].replace({r'[,]+': ' '}, regex=True, inplace=True)\n",
    "\n",
    "# Convert types\n",
    "df['description'] = df['description'].astype(str)\n",
    "df['skills_desc'] = df['skills_desc'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af521d36-c641-4b87-b623-cfc8d247e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice only top 15,000 rows\n",
    "df2 = df[:15000]\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ff473-e6cb-4995-9a24-219c13486c62",
   "metadata": {},
   "source": [
    "### Define all required step functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f77db-957d-4053-9dd1-41fe0214257a",
   "metadata": {},
   "source": [
    "def build_request(job_id, desc):\n",
    "    # check that desc is a string\n",
    "    if not isinstance(desc, str):\n",
    "        return None\n",
    "    \n",
    "    # only take the first 1000 chars of the description to limit token usage\n",
    "    trunc_desc = desc[:1000]\n",
    "\n",
    "    # build the completions request\n",
    "    request = {\n",
    "        \"custom_id\": str(job_id),\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"max_tokens\": 500,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Take the following job description and extract only 5 top keywords:\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": trunc_desc\n",
    "                }\n",
    "            ],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"KeywordList\",\n",
    "                    \"schema\": {  \n",
    "                        \"properties\": {\n",
    "                            \"keywords\": {\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"string\"\n",
    "                                },\n",
    "                                \"title\": \"Keywords\",\n",
    "                                \"type\": \"array\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"keywords\"],\n",
    "                        \"type\": \"object\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return request\n",
    "\n",
    "\n",
    "# writes a list of requests to a jsonl file\n",
    "def build_batch_file(file_path, requests):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for req in requests:\n",
    "            json_str = json.dumps(req)\n",
    "            file.write(json_str + '\\n')\n",
    "\n",
    "\n",
    "# upload the batch file to OpenAI\n",
    "def upload_batch(batch_file):\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    print(batch_input_file)\n",
    "    return batch_input_file\n",
    "\n",
    "\n",
    "# create a Batch object \n",
    "def create_batch(batch_input_file):\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": batch_input_file.filename\n",
    "        }\n",
    "    )\n",
    "    print('CREATED: {}'.format(batch))\n",
    "    return batch\n",
    "\n",
    "\n",
    "# calculate time elapsed since batch creation\n",
    "def time_elapsed(start_ts):\n",
    "    # Convert to human-readable date\n",
    "    created_datetime = datetime.fromtimestamp(start_ts)  \n",
    "    current_datetime =  datetime.fromtimestamp(time.time())  \n",
    "\n",
    "    # calculate the the difference\n",
    "    elapsed = current_datetime - created_datetime\n",
    "\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "# monitors the enqued batch process\n",
    "def process_batch(batch):\n",
    "    while True:\n",
    "        try:\n",
    "            batch = client.batches.retrieve(batch.id)\n",
    "            status = batch.status\n",
    "            print(batch)\n",
    "            if status in ['validating', 'in_progress', 'cancelling', 'finalizing']:\n",
    "                print('status: ' + status)\n",
    "                print('time elapsed: {}'.format(time_elapsed(batch.created_at)))\n",
    "            elif status in ['failed', 'cancelled', 'expired']:\n",
    "                print('status: ' + status)\n",
    "                raise Exception('Could not process batch')\n",
    "            elif status == 'completed':\n",
    "                print('status: ' + status)\n",
    "                print('time elapsed: {}'.format(time_elapsed(batch.created_at)))\n",
    "                retrieve_completions(batch)\n",
    "                return\n",
    "            else:\n",
    "                print('unkown status: ' + status)\n",
    "            time.sleep(300)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "# retrieve and save the completions response to file\n",
    "def retrieve_completions(batch):\n",
    "    if not batch.output_file_id:\n",
    "        return 'No output_file_id found'\n",
    "    file_response = client.files.content(batch.output_file_id)\n",
    "    output_file = batch.id + '.jsonl'\n",
    "    if file_response:\n",
    "        with open(output_file, 'w') as file:\n",
    "            file.write(file_response.text)\n",
    "            print(\"Responses saved to file\")\n",
    "    elif batch.error_file_id:\n",
    "        err_file_response = client.files.content(batch.error_file_id)\n",
    "        output_file = 'ERROR_' + batch.id + '.jsonl'\n",
    "        if err_file_response:\n",
    "            with open(output_file, 'w') as file:\n",
    "                file.write(err_file_response.text)\n",
    "                print(\"Errors saved to file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eae493-6308-46e4-a535-c7b94b73b816",
   "metadata": {},
   "source": [
    "### Chunk the data frame and create the batch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec4373-0ece-454e-baf8-8b3acab9e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size):\n",
    "    return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Example usage\n",
    "chunk_size = 3000\n",
    "chunks = split_dataframe(df3, chunk_size)\n",
    "\n",
    "for i, df_chunk in enumerate(chunks):\n",
    "    print(f\"Processing Chunk {i}\")\n",
    "    \n",
    "    # build all the requests\n",
    "    requests = []\n",
    "    for row in df_chunk.itertuples():\n",
    "        request = build_request(row.job_id, row.description)\n",
    "        if request:\n",
    "            requests.append(request)\n",
    "\n",
    "    # # write to file\n",
    "    output_file = './batches/batch{}.jsonl'.format(i)\n",
    "    build_batch_file(output_file, requests)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf945e5-76b3-416f-a1be-219dba03cf3d",
   "metadata": {},
   "source": [
    "### Upload all the batches to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fea6d2-212b-4871-9961-a333d2514387",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchesfiles = []\n",
    "for i in range(0, 37):\n",
    "    batch_file = './batches/batch{}.jsonl'.format(i)\n",
    "    batch = upload_batch(batch_file)\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "print('Batches count: {}'.format(len(batches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9e705-00f7-4721-a945-a467b58bae7d",
   "metadata": {},
   "source": [
    "### Create and monitor the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc9a7c-56bb-4d84-bc8d-5b4c438a6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bf in batchfiles:\n",
    "    batch = create_batch(bf)\n",
    "    try:\n",
    "        process_batch(batch)\n",
    "    except Exception as e:\n",
    "        batch_id = batch.id\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
